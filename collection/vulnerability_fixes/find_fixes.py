import csv
import sys
import pandas as pd
from ast import literal_eval
import subprocess
import json
import execjs
import multiprocessing as mp
from tqdm import tqdm
sys.setrecursionlimit(10000)

sys.path.append('..')
from finder import *  # noqa: E402
from vulnerability_fixes.graph import NetworkGraph  # noqa: E402
from vulnerability_fixes.identify import identify_applicable_vulnerabilities  # noqa: E402
from vulnerability_fixes.yarn_lock import yarn_lock_vulnerable  # noqa: E402
from vulnerability_fixes.package_json import package_json_vulnerable, package_json_workspaces  # noqa: E402
from vulnerability_fixes.package_lock_json import package_lock_json_vulnerable  # noqa: E402


class Fixer:
    def __init__(self, graph):
        self.visited = dict.fromkeys(graph.commits.keys(), False)
        self.fixed = False
        self.fixed_commits = []

    def visit(self, sha):
        self.visited[sha] = True

    def is_visited(self, sha):
        return self.visited[sha]

    def add_fixed_commit(self, sha):
        self.fixed = True
        self.fixed_commits.append(sha)


def load_advisory_database():
    database = pd.read_csv(PATH_GITHUB_ADVISORIES['modified'],
                           index_col=False,
                           dtype={
                             "package": str,
                             "ecosystem": str,
                             "severity": str,
                             "cve": str,
                             "ghsa": str,
                             "summary": str,
                             "publishedAt": str,
                             "updatedAt": str,
                             "firstPatchedVersion": str,
                             "vulnerableVersionRange": str
                           }).assign(
        publishedAt=lambda d: pd.to_datetime(d['publishedAt'], infer_datetime_format=True),
        updatedAt=lambda d: pd.to_datetime(d['updatedAt'], infer_datetime_format=True)
    )
    npm_mask = database['ecosystem'] == 'NPM'
    return database[npm_mask]


def load_update_commits():
    upd_commits = pd.read_csv(PATH_REPOSITORIES_DATA['security_updates_commits'], index_col=False)
    upd_commits['files'] = upd_commits['files'].apply(literal_eval)
    return upd_commits


def load_update_vulnerabilities():
    upd_vulns = pd.read_csv(PATH_REPOSITORIES_DATA['pr_vulnerabilities'], index_col=False)
    upd_vulns['vulnerabilities'] = upd_vulns['vulnerabilities'].apply(literal_eval)
    return upd_vulns


def associate_commits_with_vulns():
    upd_commits = load_update_commits()
    upd_vulns = load_update_vulnerabilities()
    merged = upd_commits.merge(upd_vulns, how='inner', on=['repository', 'number', 'url', 'state'])
    return merged


def filter_updates(df, mode):
    mask_merged = df['state'] == 'MERGED'
    mask_known = df['maximal_severity'].isnull()
    if mode:
        return df[mask_merged & ~mask_known].copy()
    else:
        return df[~mask_known].copy()


def is_vulnerable(database, commit_path, dirs):
    affected = False
    vulnerabilities = []
    for directory in dirs:
        is_pj_affected = False
        is_yl_affected = False
        is_plj_affected = False
        is_nsj_affected = False
        is_present = False
        constraint = []
        path_file = os.path.join(commit_path, directory, 'package.json')
        try:
            is_present, constraint, is_pj_affected, pj_vulnerabilities_ghsa, pj_vulnerabilities_ranges, \
            workspaces = package_json_vulnerable(path_file, affected_package, database, js)
            vulnerabilities.extend(pj_vulnerabilities_ghsa)
            if workspaces:
                ad_constraints = package_json_workspaces(os.path.join(commit_path, directory), workspaces,
                                                         affected_package)
                if ad_constraints:
                    is_present = True
                    constraint.extend(ad_constraints)
                    constraint = list(set(constraint))
        except (FileNotFoundError, json.JSONDecodeError):
            pass
        try:
            is_yl_affected, yl_vulnerabilities_ghsa, _ = yarn_lock_vulnerable(
                os.path.join(commit_path, directory, 'yarn.lock'), affected_package, is_present,
                constraint, database, js)
            vulnerabilities.extend(yl_vulnerabilities_ghsa)
        except (FileNotFoundError, json.JSONDecodeError, execjs._exceptions.ProgramError):
            pass
        try:
            is_plj_affected, plj_vulnerabilities_ghsa, _ = \
                package_lock_json_vulnerable(
                    os.path.join(commit_path, directory, 'package-lock.json'), affected_package,
                    is_present, database, js)
            vulnerabilities.extend(plj_vulnerabilities_ghsa)
        except (FileNotFoundError, json.JSONDecodeError):
            pass
        try:
            is_nsj_affected, nsj_vulnerabilities_ghsa, _ = \
                package_lock_json_vulnerable(
                    os.path.join(commit_path, directory, 'npm-shrinkwrap.json'), affected_package,
                    is_present, database, js)
            vulnerabilities.extend(nsj_vulnerabilities_ghsa)
        except (FileNotFoundError, json.JSONDecodeError):
            pass
        affected = affected | is_yl_affected | is_pj_affected | is_plj_affected | is_nsj_affected
    return set(vulnerabilities), affected


def find_fix(database, vulnerabilities, repository, commit, dirs, url, number, state, package, writer):
    if len(vulnerabilities) < 2:
        fixer = Fixer(dict_graphs[repository])
        fixer.visit(commit)
        vulnerability = list(vulnerabilities)[0]
        # last_commit_path = os.path.join(DIR_FREEZE, repository, dict_graphs[repository].def_branch_commits[-1])
        # _, last_commit_affected = is_vulnerable(database, last_commit_path, dirs)
        # if last_commit_affected:
        #     writer.writerow([repository, number, url, package, vulnerability, state, fixer.fixed, fixer.fixed_commits])
        # else:
        try:
            children = dict_graphs[repository].get_children(commit)
        except KeyError:  # no children
            children = []
            # writer.writerow([repository, number, url, package, vulnerability, False, []])
        try:
            for child in children:
                find_fix_recursive(database[database['ghsa'] == vulnerability], repository, child, dirs, fixer,
                                   dict_graphs[repository])
        except (RuntimeError, RecursionError) as err:
            print('Could not process - {} due to {}'.format(row['url'], err))
        writer.writerow([repository, number, url, package, vulnerability, state, fixer.fixed, fixer.fixed_commits])
    else:
        q = mp.Queue()
        processes = [mp.Process(target=find_fix_process, args=(database[database['ghsa'] == vulnerability],
                                                               vulnerability, repository, commit, dirs, url,
                                                               number, state, package, q)) for vulnerability in
                     vulnerabilities]
        for p in processes:
            p.start()
        for p in processes:
            p.join()
        for p in processes:
            writer.writerow(q.get())


def find_fix_process(database, vulnerability, repository, commit, dirs, url, number, state, package, q):
    # print(vulnerability)
    fixer = Fixer(dict_graphs[repository])
    fixer.visit(commit)
    # last_commit_path = os.path.join(DIR_FREEZE, repository, dict_graphs[repository].def_branch_commits[-1])
    # _, last_commit_affected = is_vulnerable(database, last_commit_path, dirs)
    # if last_commit_affected:
    #     # print([repository, number, url, package, vulnerability, state, fixer.fixed, fixer.fixed_commits])
    #     q.put([repository, number, url, package, vulnerability, state, fixer.fixed, fixer.fixed_commits])
    # else:
    try:
        children = dict_graphs[repository].get_children(commit)
    except KeyError:  # no children
        children = []
        # writer.writerow([repository, number, url, package, vulnerability, False, []])
    try:
        for child in children:
            find_fix_recursive(database[database['ghsa'] == vulnerability], repository, child, dirs, fixer,
                               dict_graphs[repository])
        # q.put([repository, number, url, package, vulnerability, state, fixer.fixed, fixer.fixed_commits])
    except (RuntimeError, RecursionError) as err:
        print('Could not process - {} due to {}'.format(row['url'], err))
    q.put([repository, number, url, package, vulnerability, state, fixer.fixed, fixer.fixed_commits])
    # print([repository, number, url, package, vulnerability, state, fixer.fixed, fixer.fixed_commits])
        # fixed_commits = []
        # for fixed_commit in fixer.fixed_commits:
        #     print(graph.get_famility_tree(fixed_commit))
        #     # if (not (graph.belongs_to_family(fixed_commit) in fixer.fixed_commits)) or \
        #     #         (graph.is_def_branch(fixed_commit)):
        #     #     fixed_commits.append(fixed_commit)
        # print(repository)
        # print(database[database['ghsa'] == vulnerability])
        # print(fixer.fixed)
        # print(fixer.fixed_commits)


def find_fix_recursive(database, repository, commit, dirs, fixer, graph):
    affected = True
    if not fixer.is_visited(commit):
        fixer.visit(commit)
        commit_path = os.path.join(DIR_FREEZE, repository, commit)
        if os.path.exists(commit_path):
            vulnerabilities, affected = is_vulnerable(database, commit_path, dirs)
        if affected:
            try:
                for child in graph.get_children(commit):
                    find_fix_recursive(database, repository, child, dirs, fixer, graph)
            except KeyError:  # no children
                pass
        else:
            fixer.add_fixed_commit(commit)


if __name__ == '__main__':
    js = execjs.compile("""
        const fs = require('fs');
        const lockfile = require('@yarnpkg/lockfile');
        function parse_yarn(path) {
            let file = fs.readFileSync(path, 'utf8');
            let json = JSON.stringify(lockfile.parse(file).object);
            return json;
        }
        const semver = require('semver');
        function satisfies(x, y) {
            return semver.satisfies(x, y);
        }
        function intersects(x, y) {
            return semver.intersects(x, y);
        }
    """)
    df_advisories = load_advisory_database()
    df_updates = associate_commits_with_vulns()
    df_updates = filter_updates(df_updates, False)
    df_updates = df_updates.tail(len(df_updates.index) - 1101 - 915)
    # df_updates = df_updates[df_updates['repository'] == 'ovh/manager']
    # df_updates = df_updates.head(5)
    # df_updates = df_updates.tail(len(df_updates.index)-612 - 487).head(1)
    repositories = set(df_updates['repository'].to_list())
    dict_graphs = dict()
    print('initializing network graphs...')
    for repo in tqdm(repositories):
        dict_graphs[repo] = NetworkGraph(repo)
    print('finished')
    print('performing the search...')
    with open(PATH_VULNERABILITY_FIXES['fixes'], 'w') as output:
        writer = csv.writer(output)
        writer.writerow(['repository', 'number', 'url', 'package', 'ghsa', 'state', 'fixed', 'commits'])
        for index, row in tqdm(df_updates.iterrows()):
            # print(row['url'])
            repository = row['repository']
            commit = row['parent_oid']
            commit_path = os.path.join(DIR_FREEZE, repository, commit)
            if os.path.exists(commit_path):
                date = datetime.strptime(row['date'], '%Y-%m-%dT%H:%M:%SZ')
                to_version = row['to']
                affected_package = row['package']
                # if affected_package in ['bootstrap', 'jquery']:
                #     continue
                dirs = set()
                database = identify_applicable_vulnerabilities(date, to_version, affected_package, df_advisories)
                for file in row['files']:
                    if file.split('/')[-1] in ['package.json', 'package-lock.json', 'yarn.lock', 'npm-shrinkwrap.json']:
                        dirs.add(os.path.dirname(file))
                vulnerabilities, affected = is_vulnerable(database, commit_path, dirs)
                find_fix(database, vulnerabilities, repository, commit, dirs, row['url'], row['number'], row['state'],
                         affected_package, writer)
    print('finished')
