import csv
import sys
import pandas as pd
from ast import literal_eval
import subprocess
import json
import execjs
from tqdm import tqdm

sys.path.append('..')
from finder import *  # noqa: E402
from vulnerability_fixes.identify import identify_applicable_vulnerabilities  # noqa: E402
from vulnerability_fixes.yarn_lock import yarn_lock_vulnerable  # noqa: E402
from vulnerability_fixes.package_json import package_json_vulnerable, package_json_workspaces  # noqa: E402
from vulnerability_fixes.package_lock_json import package_lock_json_vulnerable  # noqa: E402


def load_advisory_database():
    database = pd.read_csv(PATH_GITHUB_ADVISORIES['modified'],
                           index_col=False,
                           dtype={
                             "package": str,
                             "ecosystem": str,
                             "severity": str,
                             "cve": str,
                             "ghsa": str,
                             "summary": str,
                             "publishedAt": str,
                             "updatedAt": str,
                             "firstPatchedVersion": str,
                             "vulnerableVersionRange": str
                           }).assign(
        publishedAt=lambda d: pd.to_datetime(d['publishedAt'], infer_datetime_format=True),
        updatedAt=lambda d: pd.to_datetime(d['updatedAt'], infer_datetime_format=True)
    )
    npm_mask = database['ecosystem'] == 'NPM'
    return database[npm_mask]


def load_update_commits():
    upd_commits = pd.read_csv(PATH_REPOSITORIES_DATA['security_updates_commits'], index_col=False)
    upd_commits['files'] = upd_commits['files'].apply(literal_eval)
    return upd_commits


def load_update_vulnerabilities():
    upd_vulns = pd.read_csv(PATH_REPOSITORIES_DATA['pr_vulnerabilities'], index_col=False)
    upd_vulns['vulnerabilities'] = upd_vulns['vulnerabilities'].apply(literal_eval)
    return upd_vulns


def associate_commits_with_vulns():
    upd_commits = load_update_commits()
    upd_vulns = load_update_vulnerabilities()
    merged = upd_commits.merge(upd_vulns, how='inner', on=['repository', 'number', 'url', 'state'])
    return merged


def filter_updates(df, mode):
    mask_merged = df['state'] == 'MERGED'
    mask_known = df['maximal_severity'].isnull()
    if mode:
        return df[mask_merged & ~mask_known].copy()
    else:
        return df[~mask_known].copy()


def validate(df_advisories, df_updates, label, writer):
    js = execjs.compile("""
                const fs = require('fs');
                const lockfile = require('@yarnpkg/lockfile');
                function parse_yarn(path) {
                    let file = fs.readFileSync(path, 'utf8');
                    let json = JSON.stringify(lockfile.parse(file).object);
                    return json;
                }
                const semver = require('semver');
                function satisfies(x, y) {
                    return semver.satisfies(x, y);
                }
                function intersects(x, y) {
                    return semver.intersects(x, y);
                }
            """)
    correct = 0
    wrong = 0
    df_updates = filter_updates(df_updates, not label)
    for index, row in tqdm(df_updates.iterrows()):
        date = datetime.strptime(row['date'], '%Y-%m-%dT%H:%M:%SZ')
        to_version = row['to']
        database = df_advisories
        affected_package = row['package']
        repository = row['repository']
        if label:
            commit = row['parent_oid']
        else:
            commit = row['merged_oid']
        commit_path = os.path.join(DIR_FREEZE, repository, commit)
        dirs = set()
        affected = False
        vulnerabilities = []
        if os.path.exists(commit_path):
            database = identify_applicable_vulnerabilities(date, to_version, affected_package, database)
            for file in row['files']:
                if file.split('/')[-1] in ['package.json', 'package-lock.json', 'yarn.lock', 'npm-shrinkwrap.json']:
                    dirs.add(os.path.dirname(file))
            for directory in dirs:
                is_pj_affected = False
                is_yl_affected = False
                is_plj_affected = False
                is_nsj_affected = False
                is_present = False
                constraint = []
                path_file = os.path.join(commit_path, directory, 'package.json')
                try:
                    is_present, constraint, is_pj_affected, pj_vulnerabilities_ghsa, pj_vulnerabilities_ranges,\
                        workspaces = package_json_vulnerable(path_file, affected_package, database, js)
                    vulnerabilities.extend(pj_vulnerabilities_ghsa)
                    if workspaces:
                        ad_constraints = package_json_workspaces(os.path.join(commit_path, directory), workspaces,
                                                                 affected_package)
                        if ad_constraints:
                            is_present = True
                            constraint.extend(ad_constraints)
                            constraint = list(set(constraint))
                except (FileNotFoundError, json.JSONDecodeError):
                    pass
                try:
                    is_yl_affected, yl_vulnerabilities_ghsa, _ = yarn_lock_vulnerable(
                        os.path.join(commit_path, directory, 'yarn.lock'), affected_package, is_present,
                        constraint, database, js)
                    vulnerabilities.extend(yl_vulnerabilities_ghsa)
                except (FileNotFoundError, json.JSONDecodeError, execjs._exceptions.ProgramError):
                    pass
                try:
                    is_plj_affected, plj_vulnerabilities_ghsa, _ = \
                        package_lock_json_vulnerable(
                            os.path.join(commit_path, directory, 'package-lock.json'), affected_package,
                            is_present, database, js)
                    vulnerabilities.extend(plj_vulnerabilities_ghsa)
                except (FileNotFoundError, json.JSONDecodeError):
                    pass
                try:
                    is_nsj_affected, nsj_vulnerabilities_ghsa, _ = \
                        package_lock_json_vulnerable(
                            os.path.join(commit_path, directory, 'npm-shrinkwrap.json'), affected_package,
                            is_present, database, js)
                    vulnerabilities.extend(nsj_vulnerabilities_ghsa)
                except (FileNotFoundError, json.JSONDecodeError):
                    pass
                affected = affected | is_yl_affected | is_pj_affected | is_plj_affected | is_nsj_affected
            writer.writerow([row['url'], label, affected, affected_package, str(set(vulnerabilities))])
            if label:
                if affected:
                    correct += 1
                else:
                    wrong += 1
            else:
                if not affected:
                    correct += 1
                else:
                    wrong += 1
    print('correct: {}'.format(correct))
    print('wrong: {}'.format(wrong))


if __name__ == '__main__':
    with open(PATH_VULNERABILITY_FIXES['validation2'], 'w') as output:
        writer = csv.writer(output)
        writer.writerow(['pr', 'dependabot', 'predicted', 'package', 'vulnerabilities'])
        advisories = load_advisory_database()
        updates = associate_commits_with_vulns()
        validate(advisories, updates, True, writer)
        validate(advisories, updates, False, writer)


